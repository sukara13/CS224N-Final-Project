{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to premium GPUs. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to enable Premium accelerator. Subject to availability, selecting a premium GPU may grant you access to a V100 or A100 Nvidia GPU.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape dropdown. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amc7_LANktDP"
      },
      "outputs": [],
      "source": [
        "# Training step 1\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install peft\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install constants\n",
        "!pip install prompting\n",
        "!pip install trl\n",
        "!pip install huggingface_hub\n",
        "!pip install --upgrade urllib3\n",
        "!pip install --upgrade trl\n",
        "#!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WDj4537X1VO"
      },
      "outputs": [],
      "source": [
        "# Testing step 1\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install peft\n",
        "!pip install constants\n",
        "!pip install prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvLQJZi06EBM"
      },
      "outputs": [],
      "source": [
        "# Training and testing step 2\n",
        "\n",
        "# Create a datasets directory on colab and upload the train and test csv files into it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dStkQAnhiPL"
      },
      "outputs": [],
      "source": [
        "# Training and testing step 3\n",
        "\n",
        "# set train dataset size for both training and testing\n",
        "train_size = 16\n",
        "\n",
        "# set epoch size for training based on train size\n",
        "NUM_EPOCHS = 10\n",
        "if train_size == 16:\n",
        "  NUM_EPOCHS = 20\n",
        "elif train_size == 512:\n",
        "  NUM_EPOCHS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Aw_GYENn4N"
      },
      "outputs": [],
      "source": [
        "# Training and testing step 4\n",
        "\n",
        "# load section to run before both training and testing\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "from constants import *\n",
        "from prompting import *\n",
        "\n",
        "def load_base_model(model_id, api_key):\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token = api_key,\n",
        "        torch_dtype = torch.bfloat16,\n",
        "        device_map = \"auto\",\n",
        "        trust_remote_code = True\n",
        "    )\n",
        "    return base_model\n",
        "\n",
        "def load_local_model(model_id, api_key, weight_path=None):\n",
        "    model = load_base_model(model_id, api_key)\n",
        "    checkpoint_weights = f\"{weight_path}/\"\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        checkpoint_weights,\n",
        "    ).merge_and_unload()\n",
        "\n",
        "    model.config.use_cache = True\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_tokenizer(model_id, api_key):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token = api_key,\n",
        "        padding_side = \"right\",\n",
        "        add_eos_token = True,\n",
        "        add_bos_token = True,\n",
        "        trust_remote_code = True\n",
        "    )\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI8JwtVdNtrI"
      },
      "outputs": [],
      "source": [
        "# Training step 5\n",
        "\n",
        "# Fine-tune section to run for training\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from constants import *\n",
        "import pandas as pd\n",
        "\n",
        "# Constants for training arguments\n",
        "WARMUP_STEPS = 100\n",
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION = 1\n",
        "OPTIMIZER = \"adamw_torch\"\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Logging and saving arguments\n",
        "SAVE_STEPS = 10\n",
        "LOGGING_STEPS = 10\n",
        "LOG_DIR = \"./logs\"\n",
        "\n",
        "# Sequence length\n",
        "MAX_SEQUENCE_LENGTH = 150\n",
        "\n",
        "# PEFT settings\n",
        "RANK = 6\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.1\n",
        "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "\n",
        "verbose = True\n",
        "dataset_path = './datasets'\n",
        "save_weights_to = \"./saved_weights\"\n",
        "model_id = 'google/gemma-7b-it'\n",
        "api_key = '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'\n",
        "\n",
        "class FineTune():\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        device,\n",
        "        save_weights_to,\n",
        "        verbose=True\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.save_weights_to = save_weights_to\n",
        "        self.verbose = verbose\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    \"\"\"\n",
        "    This is the dataset that we'll use to finetune the model.\n",
        "    Assumes the dataset is a .csv file with columnes \"Question\" and \"Answer\".\n",
        "    \"\"\"\n",
        "    def get_dataset(self, path):\n",
        "        train_file = 'car_text_train_top_' + str(train_size) + '.csv'\n",
        "        df = pd.read_csv(f\"{path}/{train_file}\", delimiter='\\t', header=None, names=['user', 'model'])\n",
        "        df['text'] = df.apply(lambda row: f\"User: {row['user']} Model: {row['model']}\", axis=1)\n",
        "        train_dataset = Dataset.from_pandas(df[['text']])\n",
        "\n",
        "        if self.verbose: print(\"number of datapoints in train: \", len(train_dataset))\n",
        "        return train_dataset\n",
        "\n",
        "    \"\"\"\n",
        "    This loads the model, tokenizer, and peft framework\n",
        "    \"\"\"\n",
        "    def prepare_model(self, model_id, api_key):\n",
        "        # loading model\n",
        "        base_model = load_base_model(model_id, api_key)\n",
        "        base_model.config.use_cache = False\n",
        "\n",
        "        # turn on if training gives OOM errors\n",
        "        base_model.gradient_checkpointing_enable()\n",
        "\n",
        "        peft_config = LoraConfig(\n",
        "            r=RANK,\n",
        "            lora_alpha=LORA_ALPHA,\n",
        "            lora_dropout=LORA_DROPOUT,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "            target_modules=TARGET_MODULES\n",
        "        )\n",
        "        model = get_peft_model(base_model, peft_config)\n",
        "        self.model = model\n",
        "\n",
        "        # loading tokenizer\n",
        "        tokenizer = load_tokenizer(model_id, api_key)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        return peft_config\n",
        "\n",
        "    \"\"\"\n",
        "    Creates the TrainingArguments and runs SFTTrainer.\n",
        "    \"\"\"\n",
        "    def train(self, train_dataset, peft_config):\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.save_weights_to,\n",
        "            warmup_steps=WARMUP_STEPS,\n",
        "            num_train_epochs=NUM_EPOCHS,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "            gradient_checkpointing=True,\n",
        "            optim=OPTIMIZER,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            save_steps=SAVE_STEPS,\n",
        "            save_strategy=\"steps\",\n",
        "            logging_dir=LOG_DIR,\n",
        "            logging_steps=LOGGING_STEPS,\n",
        "            push_to_hub=True,\n",
        "            hub_model_id=\"sukara13/gemma7bcars\" + str(train_size),\n",
        "            hub_token='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'\n",
        "        )\n",
        "\n",
        "        trainer = SFTTrainer(\n",
        "            model=self.model,\n",
        "            train_dataset=train_dataset,\n",
        "            peft_config=peft_config,\n",
        "            max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "            tokenizer=self.tokenizer,\n",
        "            args=training_args,\n",
        "            dataset_text_field=\"text\"\n",
        "        )\n",
        "\n",
        "        # start commenting out when OOM\n",
        "        if self.verbose: print(\"=> START TRAINING!\")\n",
        "        trainer.train()\n",
        "\n",
        "        if self.verbose: print(\"=> SAVING WEIGHTS\")\n",
        "        try:\n",
        "            trainer.save_model(self.save_weights_to)\n",
        "            if self.verbose: print(f\"Weights saved to {self.save_weights_to}\")\n",
        "        except Exception as e:\n",
        "            if self.verbose: print(f\"Failed to save weights: {e}\")\n",
        "\n",
        "        # Push to hub\n",
        "        trainer.push_to_hub(\"End of training\")\n",
        "\n",
        "        \"\"\"\n",
        "        self.model.config.use_cache = True\n",
        "        self.model.eval()\n",
        "        del self.model, trainer\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        \"\"\"\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device: \", device)\n",
        "\n",
        "# Fine-tune\n",
        "FT = FineTune(\n",
        "    device = device,\n",
        "    save_weights_to = save_weights_to,\n",
        "    verbose = True\n",
        ")\n",
        "train_dataset = FT.get_dataset(dataset_path)\n",
        "peft_configs = FT.prepare_model(model_id, api_key)\n",
        "FT.train(train_dataset, peft_configs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw2JX_Er5zFd"
      },
      "outputs": [],
      "source": [
        "# Training step 6\n",
        "\n",
        "# Download the lowest loss checkpoint directory onto local machine\n",
        "# Upload those files onto HF for remote testing if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thQhvk307j1y"
      },
      "outputs": [],
      "source": [
        "# END OF TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5coQsSQ5jvC"
      },
      "outputs": [],
      "source": [
        "# Testing step 5\n",
        "\n",
        "# Create a saved_weight directory and upload the model files from the checkpoint directory on local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVy7aUtWpOgX"
      },
      "outputs": [],
      "source": [
        "# Testing step 6\n",
        "\n",
        "# Load the weights from local machine for inference\n",
        "model_id = 'google/gemma-7b-it'\n",
        "api_key = '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'\n",
        "save_weights_to = \"./saved_weights\"\n",
        "model = load_local_model(model_id, api_key, weight_path=save_weights_to + '/checkpoint-320')\n",
        "#model = load_local_model(model_id, api_key, weight_path=save_weights_to)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlJpB16u6ECZ"
      },
      "outputs": [],
      "source": [
        "# Testing step 7\n",
        "\n",
        "# Inference for a single test item from local machine to see whether tests are running fast locally\n",
        "# Note that running each inference with the fine-tuned model files uploaded to colab completes in 1-2 sec\n",
        "# The same inference take 20 seconds when the fine-tuned model files are on HF\n",
        "from datetime import datetime\n",
        "\n",
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"You are an expert car evaluator. Here are the attributes of a car in text format: Buying Price is low, Maintenance Cost is high, Doors are four, Persons are more than four, Trunk Size is big, Safety Score is high. Give your recommendation to buy this car as 'unacceptable', 'acceptable', 'good', or 'very good' without providing any explanation.\" },\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "print('encode: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=256)\n",
        "print('generate: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "output = tokenizer.decode(outputs[0]).lower().replace('\"', '')\n",
        "print(output)\n",
        "model_start = output.find(\"<start_of_turn>model\") + len(\"<start_of_turn>model\")\n",
        "model_end = output.find(\"\\n\", model_start + 2)\n",
        "model_response = output[model_start:model_end].replace('\\n', '').replace('.', '')\n",
        "print('response: ' + model_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eby2n3rWA6pO"
      },
      "outputs": [],
      "source": [
        "# Testing step 8\n",
        "\n",
        "# Test section to load local model from saved_weights\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from datetime import datetime\n",
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "model_id = 'google/gemma-7b-it'\n",
        "api_key = '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'\n",
        "save_weights_to = \"./saved_weights\"\n",
        "filePath = './datasets/'\n",
        "fineTunedModelBase = \"sukara13/gemma7bcars\"\n",
        "\n",
        "def inferenceCar(format):\n",
        "    # Set counters\n",
        "    count = 0\n",
        "    success = 0\n",
        "    fail = 0\n",
        "\n",
        "    # Read the car test data into a pandas data frame\n",
        "    testFileBase = 'car_text_test_wo_'\n",
        "    if format != 'text':\n",
        "        testFileBase = testFileBase.replace('text', format)\n",
        "    testFile = filePath + testFileBase + str(train_size) + '.csv'\n",
        "    dfTest = pd.read_csv(testFile, names=['user', 'model'], delimiter='\\t')\n",
        "    print(len(dfTest))\n",
        "    print(dfTest.head)\n",
        "\n",
        "    modelName = fineTunedModelBase + str(train_size)\n",
        "    fileName = filePath + modelName.replace('/', '_') + '-' + format + '.csv'\n",
        "\n",
        "    #print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    #model = load_local_model(model_id, api_key, weight_path=save_weights_to)\n",
        "    #tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=api_key)\n",
        "    #print('tokenizer: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    #base_model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=api_key)\n",
        "    #print('base: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "    # Load the fine-tuned model with PEFT\n",
        "    #model = PeftModel.from_pretrained(base_model, model_id, use_auth_token=api_key)\n",
        "    #print('peft: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "    with open(fileName, 'w') as file:\n",
        "        for index, row in dfTest.iterrows():\n",
        "            print(str(index) + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            user_str = row['user']\n",
        "            if format == 'table':\n",
        "              user_str = user_str.replace('|', '\\n')\n",
        "            chat = [ { \"role\": \"user\", \"content\": user_str } ]\n",
        "            prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "            print('prompt: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            print(prompt)\n",
        "            inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            print('encode: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=120)\n",
        "            print('generate: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            output = tokenizer.decode(outputs[0]).lower().replace('\"', '')\n",
        "            print('decode: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            print(output)\n",
        "\n",
        "            model_start = output.find(\"<start_of_turn>model\") + len(\"<start_of_turn>model\")\n",
        "            #model_end = output.find(\".\", model_start + 1)\n",
        "            #llm_eval = output[model_start:model_end].replace('\\n', '')\n",
        "            model_end = output.find(\"\\n\", model_start + 2)\n",
        "            llm_eval = output[model_start:model_end].replace('\\n', '').replace('.', '')\n",
        "            print('llm_eval: ' + llm_eval)\n",
        "\n",
        "            if 'unacceptable' in llm_eval:\n",
        "                llm_eval = 'unacceptable'\n",
        "            elif 'acceptable' in llm_eval:\n",
        "                llm_eval = 'acceptable'\n",
        "            elif 'very good' in llm_eval:\n",
        "                llm_eval = 'very good'\n",
        "            elif 'good' in llm_eval:\n",
        "                llm_eval = 'good'\n",
        "            else:\n",
        "                llm_eval = 'fail: ' + row['user'] + ' - ' + llm_eval\n",
        "                print(llm_eval)\n",
        "                fail += 1\n",
        "            ground_truth = row['model']\n",
        "            if llm_eval == ground_truth:\n",
        "                success += 1\n",
        "            count += 1\n",
        "            file.write(ground_truth + ',' + llm_eval + '\\n')\n",
        "            print(str(count) + \": \" + ground_truth + ',' + llm_eval)\n",
        "\n",
        "    print(fileName + ': ' + str(success) + '/' + str(count) + ' = ' + \"{:.2%}\".format(success/count))\n",
        "    print(fail)\n",
        "\n",
        "inferenceCar('text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7NMGI9ZSEf8"
      },
      "outputs": [],
      "source": [
        "# Testing step 9\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Specify the path to the file you want to download\n",
        "file_path = './datasets/sukara13_gemma7bcars' + str(train_size) + '-text.csv'\n",
        "\n",
        "# Download the file\n",
        "files.download(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0dUq1Qu33HY"
      },
      "outputs": [],
      "source": [
        "# Testing step 10\n",
        "\n",
        "# Make sure the test results csv file is downloaded onto your local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fx8iX2v7Y1p"
      },
      "outputs": [],
      "source": [
        "# END OF TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRYZbemUei9A"
      },
      "outputs": [],
      "source": [
        "# Everything below here is for backup purposes\n",
        "# Some of the code is to run inference by pointing to the fine-tuned model on HF remotely but was very slow\n",
        "# Some others with a label OLD on top are used for quantization but didn't work properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq2y4WEOBFs0"
      },
      "outputs": [],
      "source": [
        "# inference for a single item from HF\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "api_key = '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'\n",
        "model_id = \"sukara13/gemma7bcars\" + str(train_size)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=api_key)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=api_key)\n",
        "\n",
        "# Load the fine-tuned model with PEFT\n",
        "model = PeftModel.from_pretrained(base_model, model_id, use_auth_token=api_key)\n",
        "\n",
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"You are an expert car evaluator. Here are the attributes of a car in text format: Buying Price is medium, Maintenance Cost is low, Doors are three, Persons are more than four, Trunk Size is small, Safety Score is low. Give your recommendation to buy this car as 'unacceptable', 'acceptable', 'good', or 'very good' without providing any explanation.\" },\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=256)\n",
        "output = tokenizer.decode(outputs[0]).lower().replace('\"', '')\n",
        "print(output)\n",
        "model_start = output.find(\"<start_of_turn>model\") + len(\"<start_of_turn>model\")\n",
        "model_end = output.find(\".\", model_start + 1)\n",
        "model_response = output[model_start:model_end].replace('\\n', '')\n",
        "print('response: ' + model_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNLyE2wEYCjm"
      },
      "outputs": [],
      "source": [
        "# Load model from HF\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from datetime import datetime\n",
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "api_key = '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'\n",
        "fineTunedModelBase = \"sukara13/gemma7bcars\"\n",
        "filePath = './datasets/'\n",
        "testFileBase = 'car_text_test_wo_'\n",
        "\n",
        "def inferenceCar(format, trainSize):\n",
        "    # Set counters\n",
        "    count = 0\n",
        "    success = 0\n",
        "    fail = 0\n",
        "\n",
        "    # Read the car test data into a pandas data frame\n",
        "    testFile = filePath + testFileBase + str(trainSize) + '.csv'\n",
        "    dfTest = pd.read_csv(testFile, names=['user', 'model'], delimiter='\\t')\n",
        "    print(len(dfTest))\n",
        "    print(dfTest.head)\n",
        "\n",
        "    model_id = fineTunedModelBase + str(trainSize)\n",
        "    fileName = filePath + model_id.replace('/', '_') + '-' + format + '.csv'\n",
        "\n",
        "    print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=api_key)\n",
        "    print('tokenizer: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=api_key)\n",
        "    print('base: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "    # Load the fine-tuned model with PEFT\n",
        "    model = PeftModel.from_pretrained(base_model, model_id, use_auth_token=api_key)\n",
        "    print('peft: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "    \"\"\"\n",
        "    # Ensure the model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    print(model.device.type)\n",
        "    # Enable mixed precision if supported\n",
        "    if model.device.type == \"cuda\":\n",
        "        from torch.cuda.amp import autocast\n",
        "        print('cuda')\n",
        "    \"\"\"\n",
        "\n",
        "    with open(fileName, 'w') as file:\n",
        "        for index, row in dfTest.iterrows():\n",
        "            print(str(index) + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            chat = [ { \"role\": \"user\", \"content\": row['user'] } ]\n",
        "            prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "            print('prompt: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            print(prompt)\n",
        "            inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            print('encode: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=120)\n",
        "            print('generate: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            output = tokenizer.decode(outputs[0]).lower().replace('\"', '')\n",
        "            print('decode: ' + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "            print(output)\n",
        "\n",
        "            model_start = output.find(\"<start_of_turn>model\") + len(\"<start_of_turn>model\")\n",
        "            #model_end = output.find(\".\", model_start + 1)\n",
        "            #llm_eval = output[model_start:model_end].replace('\\n', '')\n",
        "            model_end = output.find(\"\\n\", model_start + 2)\n",
        "            llm_eval = output[model_start:model_end].replace('\\n', '').replace('.', '')\n",
        "            print('llm_eval: ' + llm_eval)\n",
        "\n",
        "            if 'unacceptable' in llm_eval:\n",
        "                llm_eval = 'unacceptable'\n",
        "            elif 'acceptable' in llm_eval:\n",
        "                llm_eval = 'acceptable'\n",
        "            elif 'very good' in llm_eval:\n",
        "                llm_eval = 'very good'\n",
        "            elif 'good' in llm_eval:\n",
        "                llm_eval = 'good'\n",
        "            else:\n",
        "                llm_eval = 'fail: ' + row['user'] + ' - ' + llm_eval\n",
        "                print(llm_eval)\n",
        "                fail += 1\n",
        "            ground_truth = row['model']\n",
        "            if llm_eval == ground_truth:\n",
        "                success += 1\n",
        "            count += 1\n",
        "            file.write(ground_truth + ',' + llm_eval + '\\n')\n",
        "            print(str(count) + \": \" + ground_truth + ',' + llm_eval)\n",
        "\n",
        "    print(fileName + ': ' + str(success) + '/' + str(count) + ' = ' + \"{:.2%}\".format(success/count))\n",
        "    print(fail)\n",
        "\n",
        "inferenceCar('text', 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLyLJFvQeT5b"
      },
      "outputs": [],
      "source": [
        "# OLD\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "from constants import *\n",
        "#from inference.utils import *\n",
        "from prompting import *\n",
        "import bitsandbytes as bnb\n",
        "import accelerate\n",
        "\n",
        "def load_base_model(model_id, api_key):\n",
        "    \"\"\"\n",
        "    This assumes model supports quantization\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,  # Consider your requirement\n",
        "        bnb_4bit_quant_type= \"nf4\",\n",
        "        bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    # TODO: how do these different AutoModel classes compare? Should I be calling different ones per task?\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token=api_key,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    return base_model\n",
        "\n",
        "def load_model(model_id, api_key, weight_path=None, checkpoint=0, inference=True):\n",
        "    \"\"\"\n",
        "    By default, assumes there is no finetuned weights and returns base model.\n",
        "    If there is finetuning, assumes weights were finetuned with Peft.\n",
        "\n",
        "    # TODO: change to loading from HF. Especially with inference endpoints\n",
        "    \"\"\"\n",
        "    model = load_base_model(model_id, api_key)\n",
        "    if checkpoint > 0:\n",
        "        checkpoint_weights = f\"{weight_path}/checkpoint-{checkpoint}/\"\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            checkpoint_weights,\n",
        "        ).merge_and_unload()\n",
        "    else:\n",
        "        checkpoint_weights = f\"{weight_path}/\"\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            checkpoint_weights,\n",
        "        ).merge_and_unload()\n",
        "\n",
        "    if inference:\n",
        "        model.config.use_cache = True\n",
        "        model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_tokenizer(model_id, api_key):\n",
        "    \"\"\"\n",
        "    # TODO: change to loading from HF. Especially with inference endpoints\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token = api_key,\n",
        "        padding_side=\"right\",\n",
        "        add_eos_token=True,\n",
        "        add_bos_token=True,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X4uYz5ZZxBo"
      },
      "outputs": [],
      "source": [
        "# OLD\n",
        "\n",
        "import torch\n",
        "#import argparse\n",
        "from datasets import Dataset, load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "#from loading import load_base_model, load_tokenizer\n",
        "#from inference.utils import *\n",
        "from constants import *\n",
        "import pandas as pd\n",
        "\n",
        "# Constants for training arguments\n",
        "WARMUP_STEPS = 100\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION = 1\n",
        "OPTIMIZER = \"adamw_torch\"\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Logging and saving arguments\n",
        "SAVE_STEPS = 10\n",
        "LOGGING_STEPS = 10\n",
        "LOG_DIR = \"./logs\"\n",
        "\n",
        "# Sequence length\n",
        "MAX_SEQUENCE_LENGTH = 150\n",
        "\n",
        "# PEFT settings\n",
        "RANK = 6\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.1\n",
        "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "\n",
        "verbose = True\n",
        "dataset_path = './datasets'\n",
        "save_weights_to = \"./saved_weights\"\n",
        "model_id = 'google/gemma-7b-it'\n",
        "api_key = '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'\n",
        "\n",
        "class FineTune():\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        device,\n",
        "        save_weights_to,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.save_weights_to = save_weights_to\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # these two attributes get set with prepare_model method\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    \"\"\"\n",
        "    This is the dataset that we'll use to finetune the model.\n",
        "    Assumes the dataset is a .csv file with columnes \"Question\" and \"Answer\".\n",
        "    \"\"\"\n",
        "    def get_dataset(self, path):\n",
        "        df = pd.read_csv(f\"{path}/car_text_train_top_16.csv\", delimiter='\\t', header=None, names=['user', 'model'])\n",
        "        df['text'] = df.apply(lambda row: f\"User: {row['user']} Model: {row['model']}\", axis=1)\n",
        "        train_dataset = Dataset.from_pandas(df[['text']])\n",
        "        if self.verbose: print(\"=> num datapoints: \", len(train_dataset))\n",
        "        return train_dataset\n",
        "\n",
        "    \"\"\"\n",
        "    This loads the model, tokenizer, and peft framework\n",
        "    The exact framework is as follows: base -> quantize -> peft\n",
        "    \"\"\"\n",
        "    def prepare_model(self, model_id, api_key):\n",
        "        # loading model\n",
        "        base_model = load_base_model(model_id, api_key)\n",
        "        base_model.config.use_cache = False\n",
        "        # base_model.gradient_checkpointing_enable()  # NOTE: turn on if training gives OOM errors\n",
        "\n",
        "        quantized_model = prepare_model_for_kbit_training(base_model)\n",
        "        peft_config = LoraConfig(\n",
        "            r=RANK,\n",
        "            lora_alpha=LORA_ALPHA,\n",
        "            lora_dropout=LORA_DROPOUT,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "            target_modules=TARGET_MODULES,   # query, key, value, output projections\n",
        "        )\n",
        "        model = get_peft_model(quantized_model, peft_config)\n",
        "        self.model = model.to(device) #comment out to(device)\n",
        "\n",
        "        # loading tokenizer\n",
        "        tokenizer = load_tokenizer(model_id, api_key)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        return peft_config\n",
        "\n",
        "    \"\"\"\n",
        "    Creates the TrainingArguments and runs SFTTrainer.\n",
        "    \"\"\"\n",
        "    def train(self, train_dataset, peft_config):\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.save_weights_to,\n",
        "            warmup_steps=WARMUP_STEPS,\n",
        "            num_train_epochs=NUM_EPOCHS,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "            gradient_checkpointing=True,\n",
        "            optim=OPTIMIZER,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            save_steps=SAVE_STEPS,\n",
        "            save_strategy=\"steps\",\n",
        "            logging_dir=LOG_DIR,\n",
        "            logging_steps=LOGGING_STEPS,\n",
        "            push_to_hub=True,\n",
        "            hub_model_id=\"sukara13/gemma7bcars16\",\n",
        "            hub_token='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'\n",
        "        )\n",
        "        trainer = SFTTrainer(\n",
        "            model=self.model,\n",
        "            train_dataset=train_dataset,\n",
        "            peft_config=peft_config,\n",
        "            max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "            tokenizer=self.tokenizer,\n",
        "            args=training_args,\n",
        "            dataset_text_field=\"text\"\n",
        "        )\n",
        "\n",
        "        # Train and save model locally\n",
        "        if self.verbose: print(\"=> START TRAINING!\")\n",
        "        trainer.train()\n",
        "\n",
        "        # change this path, otherwise will overwrite\n",
        "        if self.verbose: print(\"=> SAVING WEIGHTS\")\n",
        "        trainer.model.save_pretrained(self.save_weights_to)\n",
        "\n",
        "        self.model.config.use_cache = True\n",
        "        self.model.eval()\n",
        "        del self.model, trainer\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device: \", device)\n",
        "\n",
        "# Set up class and finetune\n",
        "FT = FineTune(\n",
        "    device = device,\n",
        "    save_weights_to = save_weights_to,\n",
        "    verbose = True\n",
        ")\n",
        "train_dataset = FT.get_dataset(dataset_path)\n",
        "peft_configs = FT.prepare_model(model_id, api_key)\n",
        "FT.train(train_dataset, peft_configs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}